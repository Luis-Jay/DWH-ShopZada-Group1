"""
ShopZada Data Warehouse ETL Pipeline

This DAG orchestrates the complete ETL pipeline for the ShopZada data warehouse:
1. Schema initialization and validation
2. Data ingestion from source systems
3. Staging layer transformations
4. Star schema warehouse build (dimensions & facts)
5. Data quality validation
6. Presentation layer refresh
7. Notifications and monitoring

Author: ShopZada Data Engineering Team
Created: November 2025
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.trigger_rule import TriggerRule

try:
    from airflow.providers.postgres.operators.postgres import PostgresOperator
except ImportError:  # Fall back for environments without the provider package
    from airflow.operators.postgres_operator import PostgresOperator

from datetime import datetime, timedelta
import logging
import sys
import os

# Add project paths for imports
sys.path.append('/opt/airflow/scripts/ingest')
sys.path.append('/opt/airflow/scripts/quality')

from ingest_to_postgres import ShopZadaIngestion

# Configure logging
logger = logging.getLogger(__name__)

# Default DAG arguments
default_args = {
    'owner': 'shopzada_data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email': ['data-team@shopzada.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
    'catchup': False,
    'max_active_runs': 1
}

# DAG definition
dag = DAG(
    'shopzada_dwh_etl_pipeline',
    default_args=default_args,
    description='Complete ETL pipeline for ShopZada Data Warehouse',
    schedule_interval='@daily',
    tags=['shopzada', 'dwh', 'etl', 'daily'],
    catchup=False,
    max_active_runs=1,
    dagrun_timeout=timedelta(hours=3)
)

# =============================================================================
# TASK DEFINITIONS
# =============================================================================

# Empty operators for workflow control
start_pipeline = EmptyOperator(
    task_id='start_pipeline',
    dag=dag
)

end_pipeline = EmptyOperator(
    task_id='end_pipeline',
    dag=dag,
    trigger_rule=TriggerRule.ALL_SUCCESS
)

pipeline_failed = EmptyOperator(
    task_id='pipeline_failed',
    dag=dag,
    trigger_rule=TriggerRule.ONE_FAILED
)

# ------------------------------------------------------------------------------
# 1. ENVIRONMENT VALIDATION
# ------------------------------------------------------------------------------
def validate_environment():
    """Validate that all required components are available"""
    logger.info("Validating pipeline environment...")

    # Check database connectivity
    try:
        import psycopg2
        conn = psycopg2.connect(
            host=os.getenv('DB_HOST', 'postgres'),
            port=os.getenv('DB_PORT', '5432'),
            database=os.getenv('DB_NAME', 'shopzada_dwh'),
            user=os.getenv('DB_USER', 'shopzada'),
            password=os.getenv('DB_PASSWORD', 'shopzada123')
        )
        conn.close()
        logger.info("âœ“ Database connection successful")
    except Exception as e:
        raise Exception(f"Database connection failed: {e}")

    # Check data directory
    data_dir = '/data/raw'
    if not os.path.exists(data_dir):
        logger.warning(f"Data directory {data_dir} does not exist - creating it")
        os.makedirs(data_dir, exist_ok=True)

    # Check if source files exist
    expected_files = [
        'business_department.csv',
        'customer_management.csv',
        'enterprise_department.csv',
        'marketing_transactions.csv',
        'marketing_campaigns.csv',
        'operations_department.csv'
    ]

    missing_files = []
    for file in expected_files:
        if not os.path.exists(os.path.join(data_dir, file)):
            missing_files.append(file)

    if missing_files:
        logger.warning(f"Missing source files: {missing_files}")
        logger.warning("Pipeline will continue but some ingestion tasks may fail")

    logger.info("Environment validation completed")
    return True

validate_env_task = PythonOperator(
    task_id='validate_environment',
    python_callable=validate_environment,
    dag=dag
)

# ------------------------------------------------------------------------------
# 2. SCHEMA INITIALIZATION
# ------------------------------------------------------------------------------
init_schemas_task = PostgresOperator(
    task_id='initialize_schemas',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/infra/init-scripts/01_create_schemas.sql',
    dag=dag
)

create_extensions_task = PostgresOperator(
    task_id='create_extensions',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/infra/init-scripts/02_create_extensions.sql',
    dag=dag
)

# ------------------------------------------------------------------------------
# 3. DATA INGESTION
# ------------------------------------------------------------------------------
def run_data_ingestion():
    """Execute data ingestion for all source systems"""
    logger.info("Starting data ingestion process...")

    try:
        ingestion = ShopZadaIngestion()
        results = ingestion.run_all_ingestions('/data/raw')

        # Log results
        successful = sum(1 for result in results.values() if result)
        total = len(results)

        logger.info(f"Ingestion completed: {successful}/{total} successful")

        if successful < total:
            failed_sources = [k for k, v in results.items() if not v]
            logger.warning(f"Failed sources: {failed_sources}")

        return results

    except Exception as e:
        logger.error(f"Data ingestion failed: {e}")
        raise

ingest_data_task = PythonOperator(
    task_id='ingest_source_data',
    python_callable=run_data_ingestion,
    dag=dag
)

# ------------------------------------------------------------------------------
# 4. STAGING TRANSFORMATIONS
# ------------------------------------------------------------------------------
staging_transform_task = PostgresOperator(
    task_id='staging_transformations',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/staging/load_staging.sql',
    dag=dag
)

# ------------------------------------------------------------------------------
# 5. DIMENSION LOADING (Parallel execution where possible)
# ------------------------------------------------------------------------------
# Core dimensions that don't depend on others
load_dim_date_task = PostgresOperator(
    task_id='load_dim_date',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/dim_date.sql',
    dag=dag
)

load_dim_customer_task = PostgresOperator(
    task_id='load_dim_customer',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/dim_customer.sql',
    dag=dag
)

load_dim_product_task = PostgresOperator(
    task_id='load_dim_product',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/dim_product.sql',
    dag=dag
)

# Dimensions that may depend on core dimensions
load_dim_merchant_task = PostgresOperator(
    task_id='load_dim_merchant',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/dim_merchant.sql',
    dag=dag
)

# load_dim_staff_task = PostgresOperator(
#     task_id='load_dim_staff',
#     postgres_conn_id='postgres_default',
#     sql='/opt/airflow/scripts/transform/warehouse/dim_staff.sql',
#     dag=dag
# )

load_dim_campaign_task = PostgresOperator(
    task_id='load_dim_campaign',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/dim_campaign.sql',
    dag=dag
)

# ------------------------------------------------------------------------------
# 6. FACT TABLE LOADING
# ------------------------------------------------------------------------------
load_fact_orders_task = PostgresOperator(
    task_id='load_fact_orders',
    postgres_conn_id='postgres_default',
    sql='/opt/airflow/scripts/transform/warehouse/fact_orders.sql',
    dag=dag
)

# Additional fact tables (if they exist)
# load_fact_campaign_perf_task = PostgresOperator(
#     task_id='load_fact_campaign_performance',
#     postgres_conn_id='postgres_default',
#     sql='/opt/airflow/scripts/transform/warehouse/fact_campaign_performance.sql',
#     dag=dag
# )

# load_fact_operations_task = PostgresOperator(
#     task_id='load_fact_operations',
#     postgres_conn_id='postgres_default',
#     sql='/opt/airflow/scripts/transform/warehouse/fact_operations.sql',
#     dag=dag
# )

# ------------------------------------------------------------------------------
# 7. DATA QUALITY VALIDATION
# ------------------------------------------------------------------------------
def run_data_quality_checks():
    """Execute comprehensive data quality validation"""
    logger.info("Starting data quality validation...")

    try:
        # Import the quality checker
        from dq_checks import DataQualityChecker

        checker = DataQualityChecker()
        checker.connect()

        # Run all checks
        success = checker.run_all_checks()

        # Generate and log report
        report = checker.generate_report()
        logger.info("Data Quality Report:")
        logger.info(report)

        checker.disconnect()

        if not success:
            raise Exception("Data quality checks failed - critical issues found")

        logger.info("âœ“ All data quality checks passed")
        return True

    except Exception as e:
        logger.error(f"Data quality validation failed: {e}")
        raise

data_quality_task = PythonOperator(
    task_id='data_quality_validation',
    python_callable=run_data_quality_checks,
    dag=dag
)

# ------------------------------------------------------------------------------
# 8. PRESENTATION LAYER REFRESH
# ------------------------------------------------------------------------------
refresh_presentation_task = PostgresOperator(
    task_id='refresh_presentation_layer',
    postgres_conn_id='postgres_default',
    sql="""
    -- Refresh materialized views
    SELECT presentation.refresh_daily_sales_agg();

    -- Log completion
    SELECT 'Presentation layer refresh completed at ' || NOW() as message;
    """,
    dag=dag
)

# ------------------------------------------------------------------------------
# 9. PIPELINE MONITORING & NOTIFICATIONS
# ------------------------------------------------------------------------------
def send_success_notification():
    """Send success notification"""
    logger.info("ðŸŽ‰ ShopZada DWH Pipeline completed successfully!")
    logger.info("All ETL processes completed without critical errors.")
    logger.info("Data is ready for business intelligence and reporting.")

    # In a real implementation, this could send emails, Slack messages, etc.
    return "Pipeline successful"

def send_failure_notification():
    """Send failure notification"""
    logger.error("âŒ ShopZada DWH Pipeline failed!")
    logger.error("Check Airflow logs for detailed error information.")
    logger.error("Data team has been notified via email.")

    # In a real implementation, this could send alerts, pages, etc.
    return "Pipeline failed"

success_notification_task = PythonOperator(
    task_id='success_notification',
    python_callable=send_success_notification,
    dag=dag,
    trigger_rule=TriggerRule.ALL_SUCCESS
)

failure_notification_task = PythonOperator(
    task_id='failure_notification',
    python_callable=send_failure_notification,
    dag=dag,
    trigger_rule=TriggerRule.ONE_FAILED
)

# ------------------------------------------------------------------------------
# 10. PIPELINE METRICS & LOGGING
# ------------------------------------------------------------------------------
def collect_pipeline_metrics():
    """Collect and log pipeline execution metrics"""
    logger.info("Collecting pipeline execution metrics...")

    try:
        import psycopg2

        conn = psycopg2.connect(
            host=os.getenv('DB_HOST', 'postgres'),
            port=os.getenv('DB_PORT', '5432'),
            database=os.getenv('DB_NAME', 'shopzada_dwh'),
            user=os.getenv('DB_USER', 'shopzada'),
            password=os.getenv('DB_PASSWORD', 'shopzada123')
        )

        cursor = conn.cursor()

        # Collect basic metrics
        metrics_queries = {
            'total_customers': 'SELECT COUNT(*) FROM warehouse.dim_customer',
            'total_products': 'SELECT COUNT(*) FROM warehouse.dim_product',
            'total_orders': 'SELECT COUNT(*) FROM warehouse.fact_orders',
            'total_revenue': 'SELECT SUM(net_amount) FROM warehouse.fact_orders',
            'data_quality_score': """
                SELECT ROUND(
                    (SELECT COUNT(*) FROM warehouse.fact_orders WHERE customer_key IS NOT NULL) * 100.0 /
                    NULLIF((SELECT COUNT(*) FROM warehouse.fact_orders), 0), 2
                ) as integrity_score
            """
        }

        logger.info("=== PIPELINE EXECUTION METRICS ===")
        for metric_name, query in metrics_queries.items():
            try:
                cursor.execute(query)
                result = cursor.fetchone()[0]
                logger.info(f"{metric_name}: {result}")
            except Exception as e:
                logger.warning(f"Could not collect {metric_name}: {e}")

        cursor.close()
        conn.close()

        logger.info("Pipeline metrics collection completed")
        return True

    except Exception as e:
        logger.error(f"Metrics collection failed: {e}")
        return False

pipeline_metrics_task = PythonOperator(
    task_id='collect_pipeline_metrics',
    python_callable=collect_pipeline_metrics,
    dag=dag,
    trigger_rule=TriggerRule.ALL_SUCCESS
)

# =============================================================================
# TASK DEPENDENCIES (WORKFLOW ORCHESTRATION)
# =============================================================================

# Pipeline initialization
start_pipeline >> validate_env_task >> init_schemas_task >> create_extensions_task

# Data ingestion phase
create_extensions_task >> ingest_data_task >> staging_transform_task

# Dimension loading (parallel where possible)
staging_transform_task >> load_dim_date_task
staging_transform_task >> load_dim_customer_task
staging_transform_task >> load_dim_product_task

# Dependent dimensions
load_dim_customer_task >> load_dim_merchant_task
load_dim_date_task >> load_dim_campaign_task

# Fact loading (after all dimensions are loaded)
[load_dim_merchant_task, load_dim_campaign_task] >> load_fact_orders_task

# Quality validation
load_fact_orders_task >> data_quality_task

# Presentation layer refresh
data_quality_task >> refresh_presentation_task

# Final steps and notifications
refresh_presentation_task >> pipeline_metrics_task >> success_notification_task >> end_pipeline

# Failure handling
[validate_env_task, init_schemas_task, create_extensions_task,
 ingest_data_task, staging_transform_task, load_dim_date_task,
 load_dim_customer_task, load_dim_product_task, load_dim_merchant_task,
 load_dim_campaign_task, load_fact_orders_task,
 data_quality_task, refresh_presentation_task] >> failure_notification_task

# =============================================================================
# DAG DOCUMENTATION
# =============================================================================

dag.doc_md = """
# ShopZada Data Warehouse ETL Pipeline

## Overview
This DAG executes the complete ETL pipeline for the ShopZada data warehouse, implementing a Kimball star schema with comprehensive data quality validation.

## Pipeline Stages

### 1. Environment Validation
- Database connectivity checks
- Source data availability validation
- Required directory structure verification

### 2. Schema Initialization
- Create staging, warehouse, and presentation schemas
- Set up database extensions (PostGIS, etc.)
- Configure permissions and security

### 3. Data Ingestion
- Load CSV files from all 6 source systems
- Standardize column names and data types
- Log ingestion statistics and errors

### 4. Staging Transformations
- Clean and standardize raw data
- Apply business rules and validations
- Prepare data for warehouse loading

### 5. Dimension Loading
- **dim_date**: Date dimension with calendar attributes
- **dim_customer**: Customer profiles with SCD Type 2
- **dim_product**: Product catalog with pricing
- **dim_merchant**: Seller information
- **dim_staff**: Internal staff data
- **dim_campaign**: Marketing campaign details

### 6. Fact Loading
- **fact_orders**: Core order transactions
- **fact_campaign_performance**: Campaign effectiveness metrics
- **fact_operations**: Operational performance data

### 7. Data Quality Validation
- Null value checks on critical fields
- Foreign key integrity validation
- Duplicate detection and business rule validation
- Automated severity-based alerting

### 8. Presentation Layer Refresh
- Update analytical views and materialized aggregates
- Refresh business intelligence datasets
- Optimize query performance

### 9. Monitoring & Notifications
- Collect pipeline execution metrics
- Send success/failure notifications
- Log comprehensive audit trail

## Key Features

- **Fault Tolerance**: Comprehensive error handling and retry logic
- **Parallel Processing**: Dimension loading runs in parallel where possible
- **Data Quality**: Automated validation with configurable rules
- **Monitoring**: Detailed logging and metric collection
- **Notifications**: Email alerts for pipeline status
- **Scalability**: Designed for high-volume data processing

## Dependencies

```
validate_env â†’ init_schemas â†’ create_extensions â†’ ingest_data â†’ staging_transform
                                                                             â†“
load_dimensions (parallel) â†’ load_facts â†’ data_quality â†’ refresh_presentation
                                                                             â†“
collect_metrics â†’ notifications â†’ end_pipeline
```

## Configuration

- **Schedule**: Daily at 6:00 AM
- **Timeout**: 3 hours maximum execution time
- **Retries**: 2 retry attempts with 5-minute delays
- **Concurrency**: Maximum 1 active run at a time

## Monitoring

Monitor pipeline health through:
- Airflow UI dashboard
- Log files in `/opt/airflow/logs/`
- Email notifications on failures
- Data quality dashboards in Metabase

## Troubleshooting

### Common Issues
1. **Database Connection Failures**: Check PostgreSQL container status
2. **File Not Found Errors**: Verify source data files exist in `/data/raw/`
3. **Permission Errors**: Ensure proper database user permissions
4. **Timeout Errors**: Check data volume and system resources

### Recovery Procedures
1. **Failed Ingestion**: Manually re-run ingestion task after fixing source files
2. **Dimension Load Failures**: Truncate and reload affected dimensions
3. **Quality Check Failures**: Review and fix data issues, then re-run validation

## Contacts

- **Data Engineering Team**: data-eng@shopzada.com
- **Data Quality Team**: dq-team@shopzada.com
- **Business Intelligence**: bi-team@shopzada.com

---
*DAG Version: 1.0 | Last Updated: November 2025*
"""
